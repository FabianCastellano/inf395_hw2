{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVQu6vihr_J"
      },
      "source": [
        "<center><img src=\"data:image/png;base64,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\" title=\"Title text\" /></center>\n",
        "\n",
        "\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "<h1 align='center'> INF-395/477/577 Redes Neuronales Artificiales - 2022-2 </h1>\n",
        "\n",
        "<H3 align='center'> Tarea 2: Grape disease detection  </H3>\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "**Temas**  \n",
        "* Carga de datos y preprocesamientos\n",
        "* Convoluciones y parámetros\n",
        "* Profundidad\n",
        "* Image Data Augmentation\n",
        "* Bloque Residual\n",
        "* Bloque Inception\n",
        "* Transfer Learning\n",
        "\n",
        "\n",
        "**Formalidades**  \n",
        "* Equipos de trabajo de 3 personas (*Los estudiantes deben estar preparados para presentar la tarea el día de la entrega*).\n",
        "* El entregable debe ser un _Jupyter Notebook_ incluyendo los códigos utilizados, los resultados, los gráficos realizados y comentarios. Debe seguir una estructura similar a un informe (se debe introducir los problemas a trabajar, presentar los resultados y discutirlos), se penalizará fuertemente ausencia de comentarios, explicaciones de gráficos, _etc_. Si lo prefiere puede entregar un _Jupyter Notebook_ por pregunta o uno por toda la tarea, con tal de que todos los entregables esten bien identificados y se encuentren en el mismo repositorio de _Github_.\n",
        "* Se debe preparar una presentación del trabajo realizado y sus hallazgos. El presentador será elegido aleatoriamente y deberá apoyarse en el _Jupyter Notebook_ que entregarán. \n",
        "* Formato de entrega: envı́o de link del repositorio en _Github_, al correo electrónico de los ayudantes (<maryon.morales@sansano.usm.cl>, <sebastian.sanchezl@sansano.usm.cl>), en copia al profesor (<cvalle@inf.utfsm.cl>). Especificar el siguiente asunto: [INF-395/477/577-2022-2 Tarea 2]. Invitar como colaborador a los usuarios de github \"ssanchezl\" y \"maryonmorales\" para poder acceder al repositorio en caso de ser privado.\n",
        "\n",
        "* Fecha de presentaciones 11 de Noviembre, en horario de clases.\n",
        "* Fecha de entrega: 12 de Noviembre. Hora límite de entrega: 12:00 p.m. Cualquier _commit_ luego de la hora límite no será evaluado. Se realizará descuento por atrasos en envío del mail igualmente.\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "La tarea tiene ejemplos de códigos con los cuales pueden guiarse en gran parte, sin embargo, **solo son guias** y pueden ser creativos al momento de resolver la tarea. Soluciones creativas o elegantes serán valoradas. También en algunas ocaciones se hacen elecciones arbitrarias, ustedes pueden realizar otras elecciones con tal de que haya una pequeña justificación de por qué su elección es mejor o equivalente.\n",
        "Recuerden intercalar su código con *comentarios* en celdas _Markdown_, con los comentarios de la pregunta y con cualquier análisis, fórmula (en $ \\LaTeX $) o explicación que les parezca relevante para justificar sus procedimientos. *No respondan las preguntas en comentarios en el código*.\n",
        "Noten que en general cuando se les pide elegir algo o proponer algo no se evaluará tanto la elección en si, sino que la argumentación detrás de la elección será lo más ponderado.\n",
        "Si algún modelo se demora demasiado en correr en su máquina, no olvide que puede correr _Jupyter Notebooks_ en _Collab_ de Google, incluso con la opción de aceleración con GPU (particularmente útil para los modelos más grandes), esto puede ser relevante para las máquinas más lentas al momento de realizar exploraciones con _K-folds_ o las redes más grandes. Existe también la posibilidad de utilizar _Google Cloud Plataform_ o _Amazon Web Service_, donde tienen máquinas aceleradas con GPU; maquinas ya configuradas para _deep leraning_ pueden encontrarse en el _Marketplace_ de cada proveedor de servicios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8LcJ7Zop5pB"
      },
      "source": [
        "## 1 - Redes Convolucionales para la detección de enfermedades en la uva.\n",
        "\n",
        "### Información y anexos\n",
        "\n",
        "En esta oportunidad se trabajará con un conjunto de imágenes de hojas de las plantas de uva y la tarea será identificar cuál de las 3 enfermedades esta presente en la plante o si esta sana, a partir de las imágenes entregadas. A continuación se presentan 4 sitios en para encontrar más información acerca de las enfermedades de las uvas, de las cuales se trabajará con las últimas 3 listadas:\n",
        "\n",
        "- [Diseases of Grape (Vitis spp.)](https://web.archive.org/web/20080223134516/http://www.apsnet.org/online/common/names/grape.asp)\n",
        "\n",
        "- [Black rot](http://ipm.illinois.edu/diseases/series700/rpd703/)\n",
        "\n",
        "- [Black Measles](https://ieeexplore.ieee.org/document/9397205)\n",
        "\n",
        "- [Isariopsis leaf spot](http://horticulturejournal.usamv.ro/pdf/2017/Art33.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLFE6XMBrHFe"
      },
      "source": [
        "## Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp-gNGw_rQ88"
      },
      "outputs": [],
      "source": [
        "import zipfile as zf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import filecmp as fcmp\n",
        "import os\n",
        "import scipy as sp\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import gradient_descent_v2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import History, EarlyStopping\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM0-egcQpruh"
      },
      "source": [
        "## 1.a Carga de datos y visualizaciones\n",
        "\n",
        "Establezca la ubicación de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-P3WqRIpnwZ"
      },
      "outputs": [],
      "source": [
        "data_dir  = '/content/drive/MyDrive/...\n",
        "data_zip_path = data_dir + '/GrapeVine.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDkBkh2f4kkP"
      },
      "source": [
        "Descomprima los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49dQ94HY8p-f"
      },
      "outputs": [],
      "source": [
        "with zf.ZipFile(data_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(data_dir)\n",
        "data_dir_Grapes = data_dir+ '/Grape'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1y_agom4nbu"
      },
      "source": [
        "Revise las imágenes de cada carpeta y cargue las imágenes en un arreglo $X$, el nombre de la carpeta indica el nombre de cada clase $y$. Cargue los nombres de las clases en un arreglo de tal forma que cada imagen $X$ esté asociada a su respectiva clase $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hfbltnw-8zD"
      },
      "outputs": [],
      "source": [
        "X = list()\n",
        "y = list()\n",
        "\n",
        "for dirname, _, filenames in os.walk(data_dir_Grapes):           \n",
        "    for filename in filenames: \n",
        "        image = Image.open(os.path.join(dirname, filename))\n",
        "        X.append(np.array(image, dtype=np.uint8))\n",
        "        y.append(dirname...)   \n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8aj_x1JagOW"
      },
      "source": [
        "### ¿Qué porcentaje del total de imagenes tiene cada clase? ¿Están balanceadas las clases?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma422awte3MV"
      },
      "outputs": [],
      "source": [
        "print('Per class count:')\n",
        "for cls, n in zip(*np.unique(y, return_counts=True)):\n",
        "    print(f'{cls}: {n} ({(n/float(len(y)))*100:.2f}%)')\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u53ULY2p8Xog"
      },
      "source": [
        "### Visualice algunas imágenes de cada una de las clases junto con sus nombres y revise sus dimensiones y tipo de dato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLhhNBssRsPC"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"]=[8,8]\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title(y[0])\n",
        "plt.imshow(Image.fromarray(X[0]))\n",
        " \n",
        "# ...\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_teku39XYpKu"
      },
      "source": [
        "## 1.b Preprocesamiento\n",
        "\n",
        "Transforme $y$ a encoding one hot vector, para esto utilice la librería de SciKit Learn. OneHotEncoder recibe valores categóricos numéricos, por lo que las clases en formato string deben ser transformados vía Label Encoding antes de aplicar One Hot Encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gowwWuWdor3"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uW8VL5zS7Ui"
      },
      "outputs": [],
      "source": [
        "# Creating instance of labelencoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the transformer\n",
        "le.fit(y)\n",
        "y_le = le.transform(y)\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49rzbGiodt9z"
      },
      "source": [
        "### OneHotEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAxoOKY8dxmp"
      },
      "outputs": [],
      "source": [
        "# Creating instance of onehotencoder\n",
        "ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Fit the transformer\n",
        "ohe.fit(y_le. ...)\n",
        "y_ohe = ...(ohe.transform(y_le. ...))\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shIvuW4-N9nk"
      },
      "source": [
        "### Conjuntos de datos\n",
        "Separe los datos en conjuntos de entrenamiento, validación y prueba. Elija un porcentaje para separar los conjuntos y justifique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yM5Pq47DNEv"
      },
      "outputs": [],
      "source": [
        "sf = StratifiedShuffleSplit(n_splits=1, test_size=..., random_state=42)\n",
        "\n",
        "for train_index, val_index in sf.split(X, y_ohe):\n",
        "    X_train, X_test = X[train_index], X[val_index]\n",
        "    y_train, y_test = y_ohe[train_index], y_ohe[val_index]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9xo7pW7DOW6"
      },
      "outputs": [],
      "source": [
        "sf2 = StratifiedShuffleSplit(n_splits=1, test_size=..., random_state=42)\n",
        "\n",
        "for train_index, test_index in sf2.split(X_train, y_train):\n",
        "    X_train, X_val = X_train[train_index], X_train[test_index]\n",
        "    y_train, y_val = y_train[train_index], y_train[test_index]  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkinJCfD0pgS"
      },
      "source": [
        "### Normalización de los datos\n",
        "\n",
        "Por ahora los valores de los píxeles de las imágenes se encuentran en el rango $[0,255]$. Normalice cada canal RGB por separado y obtenga $\\langle(\\mu_R,\\sigma^2_R), (\\mu_G,\\sigma^2_G), (\\mu_B,\\sigma^2_B)\\rangle_\\text{Train Set}$ y luego use estos parámetros para normalizar los 3 conjuntos separados en la preguna anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ata9mUkvm8ep"
      },
      "outputs": [],
      "source": [
        "# Perform Standarization\n",
        "def prep_normalize(X_train, X_val, X_test):\t\n",
        "\t\t\n",
        "\t\tx_means = X_train.mean(axis=(0,1,2), keepdims=True)\n",
        "\t\tx_std = X_train.std(axis=(0,1,2), keepdims=True)\n",
        "\t\t\n",
        "\t\tX_train\t= ((X_train - x_means) / x_std).astype(np.float32)\n",
        "\t\tX_val   = ((X_val - x_means) / x_std).astype(np.float32)\n",
        "\t\tX_test  = ((X_test - x_means) / x_std).astype(np.float32)\n",
        "\n",
        "\t\t# Standarize each channel separately.\n",
        "\t\treturn X_train, X_val, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWDaDu9D9ONn"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, X_test = prep_normalize(X_train, X_val, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbd8RX2xSJUq"
      },
      "source": [
        "## 1.c Primera Red Convolucional\n",
        "\n",
        "Entrenaremos una primera red convolucional sobre los datos, con la mayoría de los parámetros por defecto. Cree primero una red con la siguiente la estructura $C \\times P \\times C \\times P \\times D\\times D$, donde $C$ representa una capa convolucional, $P$ una capa Max Pooling y $D$ una capa densa. Note que antes de la capa densa debe agregar una capa Flatten que transforma los filtros a vectores que luego pueden ser utilizados por la capa densa.\n",
        "\n",
        "Para los parámetros de las capas, fijaremos ambas capas convolucionales con 128 filtros de $3 \\times 3$, stride por default de $1 \\times 1$, y padding \"same\", es decir, agregaremos $0$ a los bordes de la imágen de tal manera que se preserve la dimension de la imágen al atravesar la capa, las capas de pooling tendrán tamaño y stride $2\\times 2$, como muentra el código. \n",
        "\n",
        "Utilice el método `.summary` del modelo para ver la cantidad de parámetros y las dimensiones de los outputs de cada capa. Justifique el número de parámetros y el Output Shape de cada capa en función de la estructura de la red y lo aprendido en clase.\n",
        "\n",
        "Preguntas:\n",
        "\n",
        "Investigue y explique qué es el stride en la capa convolucional, ¿Qué operación efectúa la activación SoftMax y qué representaría en términos del problema el vector de salida de la red?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jLEuYPhRUy2"
      },
      "outputs": [],
      "source": [
        "def baseline_model(inputs,num_classes):    \n",
        "    x = inputs\n",
        "    # create model    \n",
        "    conv1  = Conv2D(filters = 128,\n",
        "                    kernel_size = (3,3),\n",
        "                    strides=(1, 1),\n",
        "                    padding = 'same',\n",
        "                    activation = 'relu',\n",
        "                    data_format=\"channels_last\")(x)\n",
        "\n",
        "    pool1  = MaxPooling2D(pool_size = (2,2))(conv1)\n",
        "\n",
        "    conv2  = Conv2D(filters = 128,\n",
        "                    kernel_size = (3,3),\n",
        "                    strides=(1, 1),\n",
        "                    padding = 'same',\n",
        "                    activation = 'relu')(pool1)\n",
        "\n",
        "    pool2  = MaxPooling2D(pool_size = (2,2))(conv2)    \n",
        "\n",
        "    dense1 = Dense(units=32, activation='relu')(Flatten()(pool2))    \n",
        "\n",
        "    dense2 = Dense(units=num_classes, activation='softmax')(dense1)\n",
        "\n",
        "    model  = Model(inputs = inputs, outputs = dense2)    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnqKv3PaIaYF"
      },
      "source": [
        "Compile la red definida en el item anterior. Para esta pregunta puede usar la activación, el optimizador, learning rate y learning rate decay que usted desee con sus respectivos argumentos, justificando su elección, y debe usar como loss Categorical Crossentropy.\n",
        "\n",
        "Entrene la red hasta observar convergencia recuperando su history. Grafique como varía el accuracy en entrenamiento y validación a lo largo del aprendizaje. Adicionalmente obtenga el accuracy para todo el conjunto de test.\n",
        "\n",
        "Preguntas: ¿Por qué preferimos medir crossentropy y no por ejemplo MSE en este problema?¿Qué valor representa el accuracy?, ¿Le parece buena medida de desempeño para este problema?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiR0SsEWIefO"
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=X_train.shape[1:])\n",
        "num_classes = 4\n",
        "\n",
        "# build the model\n",
        "model = baseline_model(inputs,num_classes)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "my_callbacks = [History(), EarlyStopping(patience=10, min_delta=0.01, monitor='val_accuracy', mode=\"max\" restore_best_weights = True)]\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=..., batch_size=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_IiB-oXaPDq"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"]=[14,4]\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label=\"Training accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label=\"Validation accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label=\"Training loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation loss\")\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvYfW7AqhgnW"
      },
      "outputs": [],
      "source": [
        "# Test evaluation of the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSsjXpDfm0Vz"
      },
      "outputs": [],
      "source": [
        "print(f'Loss in test data: {test_loss}\\nAccuracy in test data: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp7MviNiHtU8"
      },
      "source": [
        "## 1.d Parámetros de la capa Convolucional\n",
        "\n",
        "Entrene la misma red (arquitectura $C\\times P\\times C\\times P\\times D\\times D$) de la pregunta anterior. A continuación en cada punto debe probar las variaciones que se le indican, manteniendo el resto tal como estaba. Por ejemplo si está probando distintos números de filtros, solo se modifica el número de filtros; si está probando distintos kernel size, entonces el número de filtros volverá a fijarse en 128 como estaba en la pregunta anterior.\n",
        "\n",
        "+ filters (pruebe 2 números, ej: 512, 32),\n",
        "+ kernel_size (pruebe $1\\times 1$, $5\\times 5$, $7\\times 7$ y $9\\times 9$),\n",
        "+ strides (pruebe (2,2) y (3,3))\n",
        "+ padding (pruebe valid),\n",
        "+ dilation_rate (pruebe 1,2,3)\n",
        "+ MaxPooling y AveragePooling (en ambos casos pruebe stride de (3,3), (5,5))\n",
        "+ Dense (pruebe con 2 capas densas con 2 números de neuronas distintos en cada capa (sin contar la de salida softmax de 4 clases), por ejemplo: \n",
        "$$\\cdots \\times D(units=64)\\times D(units=32)\\times D(\\text{num_clases}, ``softmax\"),$$\n",
        "$$\\cdots \\times D(units=128)\\times D(units=8)\\times D(\\text{num_clases}, ``softmax\"),$$\n",
        "$$\\cdots \\times D(units=256)\\times D(\\text{num_clases}, ``softmax\"),$$\n",
        "$$\\cdots \\times D(units=16)\\times D(\\text{num_clases}, ``softmax\").$$\n",
        "\n",
        "Lea la documentación de esta herramienta e investigue cuales de estos parámetros se pueden combinar para realizar una búsqueda en grilla (¿Qué es una búsqueda en grilla?) y cuales no son compatibles entre ellos para ciertos valores, explique por qué.\n",
        "\n",
        "Formule la dimensión de salida $(H_{out}, W_{out})$ en función de la dimensión de entrada $(H_{in}, W_{in})$, el tamaño del kernel $k$, el stride $s$, el padding $p$ y el dilation_rate $d$.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7meSbqRRZ0tO"
      },
      "outputs": [],
      "source": [
        "def p_model(params):\n",
        "    conv1  = Conv2D(...)(x)\n",
        "\n",
        "    pool1  = ...Pooling(...)(conv1)\n",
        "\n",
        "    conv2  = Conv2D(...)(pool1)\n",
        "\n",
        "    pool2  =...Pooling(...)(conv2)    \n",
        "\n",
        "    dense_I = Dense(...)(Flatten()(pool2))    \n",
        "\n",
        "    #...\n",
        "    \n",
        "    dense_O = Dense(units=num_classes, activation='softmax')(dense_I)\n",
        "\n",
        "    model  = Model(inputs = inputs, outputs = dense_O)  \n",
        "\n",
        "    return model\n",
        "\n",
        "inputs = Input(shape=X_train.shape[1:])\n",
        "num_classes = 4\n",
        "\n",
        "params = ...\n",
        "\n",
        "for ...:\n",
        "\n",
        "    # build the model\n",
        "    model = p_model(params)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    my_callbacks = [History(), EarlyStopping(patience=10, min_delta=0.01, monitor='val_accuracy', mode=\"max\" restore_best_weights = True)]\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=, batch_size=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlOPggdZNgVv"
      },
      "source": [
        "## 1.e Bloque $C\\times C\\times P$\n",
        "\n",
        "Cree y entrene redes utilizando *bloques* $C\\times C\\times P$ (dos capas convolucionales seguida de una de maxpool) y luego dos capas densas, por ejemplo la estructura de una red de 2 *bloques* sería la siguiente: $\\underbrace{C\\times C\\times P}_{\\text{bloque 1}} \\times \\underbrace{C\\times C\\times P}_{\\text{bloque 2}}\\times D \\times D$. Puede agregar las capas densas que desee basandose en la experiencia de las preguntas anteriores o lo aprendido en clases (justifique).\n",
        "\n",
        "La idea es explorar qué ocurre a medida que se modifica la profundidad de la red. Para esto, entrene redes con distintos números de *bloques*. Debe a lo menos entrenar una red por cada número de *bloques*: entre 1 y 5 *bloques*.\n",
        "\n",
        "Comente sobre los dos casos extremos (1 y 5 *bloques*), ¿le parece que alguno de los dos sea buena aproximación para la clasificación de estas imágenes? Para cada red recupere history y grafique los valores de accuracy en entrenamiento y validación.\n",
        "\n",
        "Quedan a su discreción los parámetros de cada capa convolucional, sin embargo, para el número de filtros en las capas convolucionales se recomienda disminuir el número de filtros por la mitad en cada *bloque* por ejemplo: $C(filters=128)\\times C(filters=128)\\times P \\times C(filters=64)\\times C(filters=64)\\times P\\times \\cdots \\times D \\times \\cdots \\times D.$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QAIhfsxZ1-5"
      },
      "outputs": [],
      "source": [
        "for bloque in n_bloques:\n",
        "    model.add(Conv2D(...))\n",
        "    model.add( Conv2D(...))\n",
        "    model.add(...Pooling(...))\n",
        "model.add(flatten())\n",
        "model.add(Dense(...))\n",
        "model.add(Dense(...))\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-HNQ18OpRzS"
      },
      "source": [
        "## 1.f Data Augmentation\n",
        "\n",
        "Otra manera de evitar sobreajuste y mejorar los desempeños de una red convolucional es usar aumentación de datos. La idea detrás de este método es un hecho muy simple: si rotamos ligeramente una foto por ejemplo de un caballo, seguirá siendo de un caballo. Lo mismo si la movemos ligeramente hacia algun lado, hacia arriba, etc.\n",
        "\n",
        "Keras trae implementado un generador de imágenes aumentadas como se muestra en el *código* a continuación. Explore a lo menos 4 variaciones del generador a continuación, la elección de los parámetros y sus respectivos valores queda en sus manos). Una vez generada la data aumentada, entrene la mejor red que haya obtenido a lo largo de toda la tarea.\n",
        "\n",
        "Pregunta: ¿Mejora el desempeño de la red utilizando aumentación de datos?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLXVe-97pJQj"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=...,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=...,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=...,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=...,  # divide each input by its std\n",
        "        zca_whitening=...,  # apply ZCA whitening\n",
        "        rotation_range=...,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = ..., # Randomly zoom image \n",
        "        width_shift_range=...,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=...,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=...,  # randomly flip images\n",
        "        vertical_flip=...)  # randomly flip images\n",
        "\n",
        "datagen.fit()\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTBpC5H-pJvg"
      },
      "source": [
        "## 1.g Bloque Residual\n",
        "\n",
        "A continuación se probaran arquitecturas de red con conexiones residuales, puede usar la data aumentada si le parece indicado.\n",
        "\n",
        "<center><img src=\"https://production-media.paperswithcode.com/methods/resnet-e1548261477164.png\" width=\"300\"/></center>\n",
        "\n",
        "\n",
        "En la imagen se muestra el bloque básico de la arquitectura de la primera variación de la Red Residual: ResNet (https://arxiv.org/abs/1512.03385). Investigue las principales motivaciones de cómo implementar este tipo de red, sus ventajas, sus desventajas y luego cree y entrene una red con 1, 2, 3, 4 y 5 bloques residuales. ¿Como deben ser las dimensiones del input $x$ y de $\\mathcal{F}(x)$ para poder realizar la operación $\\mathcal{F}(x) + x$ entes del *relu* de más abajo?, ¿qué tipo de padding hay que usar en la convolución para mantener las mismas dimensiones de entrada y de salida?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEWeM2s3piLb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Add, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def relu_bn(...):\n",
        "    relu = ReLU()(inputs)\n",
        "    bn = BatchNormalization()(relu)\n",
        "    return bn\n",
        "\n",
        "def residual_block(x, filters, kernel_size) -> Tensor:\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides= (1 if not downsample else 2),\n",
        "               filters=filters,\n",
        "               padding=\"same\")(x)\n",
        "    y = relu_bn(y)\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides=1,\n",
        "               filters=filters,\n",
        "               padding=\"same\")(y)\n",
        "\n",
        "    # ...\n",
        "    out = Add()([x, y])\n",
        "    out = relu_bn(out)\n",
        "    return out\n",
        "\n",
        "def create_res_net():\n",
        "    \n",
        "    inputs = Input(shape=(...))\n",
        "    num_filters = 64\n",
        "    \n",
        "    t = BatchNormalization()(inputs)\n",
        "    t = Conv2D(kernel_size=3,\n",
        "               strides=1,\n",
        "               filters=num_filters,\n",
        "               padding=\"same\")(t)\n",
        "    t = relu_bn(t)\n",
        "    \n",
        "    num_blocks_list = [...]\n",
        "    for i in range(len(num_blocks_list)):\n",
        "        #...\n",
        "        t = residual_block(t, ..., filters=num_filters)\n",
        "        num_filters *= 2\n",
        "        #...\n",
        "    \n",
        "    t = ...Pooling2D(4)(t)\n",
        "    t = Flatten()(t)\n",
        "    outputs = Dense(4, activation='softmax')(t)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5M-v8rSpj5J"
      },
      "source": [
        "## 1.h Bloque Inception\n",
        "\n",
        "En preguntas anteriores se pidió probar con distintos tamaños de kernel, en este tipo de bloque la idea es usar varios tamaños al mismo tiempo y así extraer características de contextos grandes y pequeños.\n",
        "\n",
        "Para comenzar es útil preguntarse: ¿Qué pasa si se desea modificar solamente el número de canales?. Existe una forma de mantener las dimensiones de entrada, modificando el número de canales de la salida: La convolución $1 \\times 1$.\n",
        "\n",
        "Imagine que tiene una matriz $M$ de tamaño $6 \\times 6$, si realiza una convolución con una matriz $m$ de $1\\times 1$ y luego una activación, entonces $M^{6 \\times 6} * m^{1\\times 1}$ será una multiplicación element-wise entre el valor de $m$ y cada elemento de $M$ resultando luego de la activación otra matriz de $6\\times 6$. \n",
        "\n",
        "Teniendo en mente lo anterior, suponga ahora que la entrada es un arreglo de tamaño $(6 \\times 6 \\times 32)$, es decir, tiene 32 canales. Considere que desea aplicar una convolución con $N$ filtros a todos los canales, entonces el tamaño de los filtros debe ser $(1\\times 1 \\times 32)$, el resultado de la convolución será tendrá un tamaño $(6 \\times 6 \\times N)$. Por lo tanto es posible aumentar, mantener o disminuir el la cantidad de canales de la salida. La idea de mantener los canales es aplicar una activación no lineal, lo que permite a la red aprender funciónes más complejas.\n",
        "\n",
        "Otra ventaja de la convolución $(1\\times 1)$ es que ayuda a reducir \n",
        "el costo computacional entre otras convoluciones, por ejemplo si se aplican 32 filtros de tamaño $(5\\times 5 \\times 192)$ con padding `same` a una entrada de tamaño $(28 \\times 28 \\times 192)$, entonces la salida será de tamaño $(28 \\times 28 \\times 32)$ y el total de multiplicaciones será: $28\\times 28 \\times 32 \\times 5 \\times 5 \\times 192 = 120\\; \\text{Millones}$. Por esta razón al aplicar convoluciones con filtros de distintos tamaños conviene reducir el número de operaciones a través de la siguiente idea llamada \"cuello de botella\":\n",
        "\n",
        "- A la entrada de $(28 \\times 28 \\times 192)$ aplique 16 filtros convolucionales de $(1 \\times 1 \\times 192)$, obteniendo una salida de $(28 \\times 28 \\times 16)$.\n",
        "\n",
        "- Luego aplique 32 filtros convolucionales de $(5 \\times 5 \\times 16)$, obteniendo una salida de $(28 \\times 28 \\times 32)$.\n",
        "\n",
        "Para la primera operación se requieren $28 \\times 28 \\times 16 \\times 192 = 2.4\\; \\text{Millones}$ de multiplicaciones, mientras que en la segunda son $28 \\times 28 \\times 32 \\times 5 \\times 5 \\times 16 = 10\\; \\text{Millones}$. Finalmente sumando ambas cantidades se obtiene un total de $12.4\\; \\text{Millones}$ de multiplicaciones, casi un 10% de lo que se obtiene al realizar la operación directamente!.\n",
        "\n",
        "Con todo lo anterior en mente se le pide que implemente una red con una red utilizando el bloque presentado en la imagen de abajo. Pruebe con 1,3 y 4 bloques antes de aplicar las capas densas. Note que en la imagen la entrada y la salida conservan la dimensión de $(28 \\times 28)$, por lo tanto entre cada bloque *inception* puede ir bajando dicha dimensión, para obtener más información sobre la construcción del bloque puede leer el siguiente documento https://arxiv.org/pdf/1409.4842.pdf.\n",
        "\n",
        "\n",
        "<center><img src=\"https://blog.kakaocdn.net/dn/dvyzrN/btqNkQRUokj/DrrKv0t5QJ9CyRI45aosd1/img.jpg\" width=\"300\"/><center/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IncYP7uLpvvd"
      },
      "outputs": [],
      "source": [
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5e3w6lrpv_R"
      },
      "source": [
        "## 1.i Transfer Learning\n",
        "\n",
        "A continuación se le pide escoger 1 modelo pre entrenado de entre los presentes en la siguiente lista https://keras.io/api/applications/, puede usar alguno de los vistos a lo largo de la tarea o puede escoger otro que le parezca. En el link anterior y en el siguiente puede encontrar información acerca de los modelos, su implementación y otras consideraciones.\n",
        "\n",
        "Preguntas: ¿Qué es Transfer Learning?, ¿Cómo se implementa Transfer Learning con un modelo pre entrenado? (explique brevemente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaljxP6fQ1UD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# and a logistic layer -- let's say we have 200 classes\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "# train the model on the new data for a few epochs\n",
        "model.fit(...)\n",
        "\n",
        "# at this point, the top layers are well trained and we can start fine-tuning\n",
        "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
        "# and train the remaining top layers.\n",
        "\n",
        "# let's visualize layer names and layer indices to see how many layers\n",
        "# we should freeze:\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)\n",
        "\n",
        "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 249 layers and unfreeze the rest:\n",
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "# we need to recompile the model for these modifications to take effect\n",
        "# we use SGD with a low learning rate\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
        "\n",
        "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
        "# alongside the top Dense layers\n",
        "model.fit(...)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fLFE6XMBrHFe",
        "DM0-egcQpruh",
        "3gowwWuWdor3",
        "49rzbGiodt9z",
        "shIvuW4-N9nk",
        "AkinJCfD0pgS",
        "Jbd8RX2xSJUq",
        "Qp7MviNiHtU8",
        "hlOPggdZNgVv",
        "B-HNQ18OpRzS",
        "WTBpC5H-pJvg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}